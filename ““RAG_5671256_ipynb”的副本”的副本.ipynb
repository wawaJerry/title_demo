{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 在 Colab 第一个代码块，运行一次即可\n",
        "!pip install -q \"transformers>=4.39\" ctransformers sentence-transformers \\\n",
        "                 langchain langchain-community chromadb rank_bm25"
      ],
      "metadata": {
        "id": "OzsdctO-ZzO3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 块 2：全局配置（修改这里） ===\n",
        "DATA_PATH = \"item_desc_dataset.txt\"   # TSV 文件，每行：标题 \\t 描述\n",
        "\n",
        "# 样本量：设为 0 或负数，或大于总行数时，读取全部；否则随机抽 SAMPLE_SIZE 行\n",
        "SAMPLE_SIZE = 1000     # ← 在 0～2129187 之间修改\n",
        "\n",
        "# 用户只需修改引号里的关键词，就能改变检索输入\n",
        "QUERY = \"夹克\"   # ← 修改这里\n"
      ],
      "metadata": {
        "id": "TkW79OdOvmds"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 块 3：导入所有依赖 ===\n",
        "import os, random, shutil, textwrap, jieba\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.retrievers import BM25Retriever\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, logging as hf_logging\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "hf_logging.set_verbosity_error()  # 关闭 HF 过多日志\n"
      ],
      "metadata": {
        "id": "zbWIKAuWvpD3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 块 4：加载 & 抽样数据 ===\n",
        "# 读取所有行\n",
        "with open(DATA_PATH, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "    lines = f.readlines()\n",
        "# 随机抽样逻辑\n",
        "random.seed(42)\n",
        "if SAMPLE_SIZE > 0 and len(lines) > SAMPLE_SIZE:\n",
        "    lines = random.sample(lines, SAMPLE_SIZE)\n",
        "\n",
        "# 构建 Document 列表\n",
        "docs = []\n",
        "for line in lines:\n",
        "    if \"\\t\" not in line:\n",
        "        continue\n",
        "    title, desc = line.strip().split(\"\\t\", 1)\n",
        "    docs.append(Document(page_content=f\"{title}\\n{desc}\"))\n",
        "\n",
        "print(f\"[Step 1] 读取并抽样数据 → 共 {len(docs)} 条记录\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLenU1m8vrFY",
        "outputId": "35aea63e-1c8b-4983-a20e-b5b513c5487b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Step 1] 读取并抽样数据 → 共 1000 条记录\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 块 5：构建向量库 & BM25 索引 ===\n",
        "# 文本切块\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=50)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "# 加载嵌入模型\n",
        "embed_model = \"BAAI/bge-small-zh-v1.5\"\n",
        "print(f\"[Step 2] 加载嵌入模型: {embed_model}\")\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embed_model)\n",
        "\n",
        "# 清理&创建持久化目录\n",
        "persist_dir = \"./chroma_db\"\n",
        "if os.path.isdir(persist_dir):\n",
        "    shutil.rmtree(persist_dir)\n",
        "os.makedirs(persist_dir, exist_ok=True)\n",
        "\n",
        "# 构建 Chroma 向量库\n",
        "vectordb = Chroma.from_documents(\n",
        "    chunks, embeddings, persist_directory=persist_dir\n",
        ")\n",
        "\n",
        "# 构建 BM25 检索器\n",
        "bm25 = BM25Retriever.from_documents(chunks)\n",
        "bm25.k = 5\n",
        "\n",
        "print(f\"[Step 2] 已构建向量库 & BM25，共切分出 {len(chunks)} 段文本\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuGjbuokwPkS",
        "outputId": "dd67597b-cb33-4869-97ea-293b5e275d10"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Step 2] 加载嵌入模型: BAAI/bge-small-zh-v1.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-3bc6b42b9862>:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=embed_model)\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Step 2] 已构建向量库 & BM25，共切分出 1000 段文本\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 块 6：加载 Qwen3-0.6B 模型 ===\n",
        "model_name = \"Qwen/Qwen3-0.6B\"\n",
        "print(f\"[Step 3] 加载 LLM: {model_name}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, trust_remote_code=True, device_map=\"auto\"\n",
        ")\n",
        "print(\"[Step 3] 模型就绪\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RabkcEdqwSGw",
        "outputId": "6aeffe8f-92e1-403d-b860-56266ac0f367"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Step 3] 加载 LLM: Qwen/Qwen3-0.6B\n",
            "[Step 3] 模型就绪\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 改进后的混合检索代码片段 ===\n",
        "\n",
        "# 1. 先用 jieba 对所有 chunks 分词，准备 BM25\n",
        "texts = [chunk.page_content for chunk in chunks]\n",
        "tokenized_corpus = [list(jieba.cut(t)) for t in texts]\n",
        "bm25_okapi = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "# 2. BM25 检索并归一化得分（分数越高越相关）\n",
        "query_tokens = list(jieba.cut(QUERY))\n",
        "bm25_raw_scores = bm25_okapi.get_scores(query_tokens)\n",
        "# 归一化：score / max_score\n",
        "max_bm = max(bm25_raw_scores) or 1.0\n",
        "bm25_scores = [s / max_bm for s in bm25_raw_scores]\n",
        "\n",
        "# 3. 向量检索并归一化得分（Chroma 返回的是距离，越小越相关）\n",
        "vec_results = vectordb.similarity_search_with_score(QUERY, k=5)\n",
        "# similarity_search_with_score 返回 [(doc, distance), ...]\n",
        "# 转换成“相关度分数”：score = 1 / (1 + distance)\n",
        "vec_scores = { d.page_content: 1/(1+dist) for d, dist in vec_results }\n",
        "\n",
        "# 4. 加权融合：这里简单平均，也可以用别的权重\n",
        "combined_scores = {}\n",
        "for idx, doc in enumerate(chunks):\n",
        "    text = doc.page_content\n",
        "    combined_scores[text] = bm25_scores[idx]  # BM25 部分\n",
        "\n",
        "for doc, sim_score in vec_scores.items():\n",
        "    combined_scores[doc] = combined_scores.get(doc, 0) + sim_score\n",
        "\n",
        "# 5. 排序 & 取 top5\n",
        "sorted_docs = sorted(\n",
        "    combined_scores.items(),\n",
        "    key=lambda kv: kv[1],\n",
        "    reverse=True\n",
        ")\n",
        "top5 = [Document(page_content=txt) for txt, _ in sorted_docs[:5]]\n",
        "\n",
        "# 6. 打印结果\n",
        "print(f\"[Step 4] 混合检索后 Top 5：\")\n",
        "for i, d in enumerate(top5, 1):\n",
        "    snippet = d.page_content.replace(\"\\n\", \" \")[:200]\n",
        "    print(f\"[{i}] {snippet}…\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzH4skJzw7jC",
        "outputId": "e8acf70b-dd6e-4c7e-8b05-d285cd69e411"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.985 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.985 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Step 4] 混合检索后 Top 5：\n",
            "[1] dumb秋季新款刺绣夹克ulzzang男士外套港风bf宽松上衣学生外衣潮 dumb秋冬加绒刺绣夹克ulzzang男士外套港风bf宽松上衣学生外衣潮。90后懒人显瘦潮流都市雅痞约会工作撩妹夹克…\n",
            "[2] 千纸鹤夹克2018秋冬季男士青年商务纯色外套飞行休闲夹克潮男3257 时尚潮流资讯型男撞色潮流夹克，优质的面料具有很好的防紫外线和吸湿排汗的功效，炫酷的外表，宽松的版型，显得更加随心，独特的设计，更显时尚、潮流，搭配深色休闲裤和时尚休闲鞋，成熟之中透露着活力…\n",
            "[3] 男士外套2018新款春秋冬季夹克韩版帅气男装秋装秋冬加绒加厚上衣 大热军事风型男复古潮流夹克，全棉材质，舒适而又富有弹性，款式新颖，举手投足间尽显绅士气度，时尚的设计，细腻的做工，你值得拥有…\n",
            "[4] 七匹狼夹克男2018秋冬季新款棒球服男士外套中年爸爸休闲上衣1808 型男原创印花潮流大叔夹克，质感面料，穿出时尚潮流气质，精致做工，上身舒适，毫无束缚感，采用简约而不简单的设计，十分的休闲百搭，时尚百搭，适合各种身材…\n",
            "[5] 布衣传说连帽中长款夹克男装秋季韩版修身青年外套上衣服潮流褂子 型男学生原创街拍男士秋季外套，面料很舒服，穿着显身材，做工精细美观，凸显优雅纯品，精致的做工，实用性与美观性完美结合，上身显得身材比例超好，低调中带着酷劲…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 块 8：基于检索结果，生成 3 组【标题 + 描述】 ===\n",
        "context = \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "context\n",
        "\n",
        "QUERY\n",
        "\n",
        "prompt = textwrap.dedent(f\"\"\"\n",
        "你是一位精通电商爆款文案的资深运营。根据检索到的商品资料，为【{QUERY}】生成3组差异化【标题 + 产品描述】，需严格遵循：\n",
        "\n",
        "【标题规则】\n",
        "28-30字无标点关键词组合，必须包含：\n",
        "季节属性 | 核心痛点 | 材质工艺 | 人群定位 | 风格元素\n",
        "（参考案例结构：\"秋冬1000d加绒加厚微压连裤袜显瘦美腿打底袜女哑光保暖神器\"）\n",
        "\n",
        "【描述规则】\n",
        "80-100字分4个模块：\n",
        "1. 工艺细节（原料/技术/专利）\n",
        "2. 场景痛点（保暖/显瘦/防勾丝）\n",
        "3. 对比优势（与同类产品差异化）\n",
        "4. 情感价值（自信/魅力/舒适体验）\n",
        "（参考：\"高密度天鹅绒叠加微压技术显瘦不紧绷脚踝薄绒处理搭配单鞋不臃肿...\"）\n",
        "\n",
        "【格式要求】\n",
        "严格使用数字编号+缩进格式，样例：\n",
        "1. 标题：秋冬加厚300g磨毛工装裤男束脚小脚裤冬季防风加绒休闲裤韩版潮流\n",
        "   描述：采用300g海岛棉磨毛工艺锁温效果提升40%,零下15℃单裤过冬告别臃肿叠穿,膝盖立体裁片比普通工装裤活动更自如,做旧金属扣与抽绳设计打造高街潮男气质\n",
        "\n",
        "参考检索内容：\n",
        "{context}\n",
        "\"\"\").strip()\n",
        "\n",
        "# 调用模型\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "text     = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs   = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "out_ids  = model.generate(**inputs, max_new_tokens=512)[0][len(inputs.input_ids[0]):]\n",
        "result   = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n[Step 5] 生成文案结果：\\n\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "OfG_DYbxycl6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}